<html>
<head>
<title>GUCKERT-Lab4.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #2aacb8;}
.s4 { color: #cf8e6d;}
.s5 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
GUCKERT-Lab4.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1"># Practical session 4 - K-nearest neighbours (K-NN) classification with numpy, scikit-learn, cython and numba 
 
Student: 
- Mathis Guckert([link](https://github.com/MGuckert)) 
</span><span class="s0">#%% md 
</span><span class="s1">**Useful references for this lab**: 
 
[1] scikit-learn: [documentation](https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn%20classification) 
 
[2] `numba`: [documentation](http://numba.pydata.org/)  
 
[3] cython: [a very useful tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial), and [another one](http://docs.cython.org/en/latest/src/tutorial/cython_tutorial.html) 
 
 
 
## &lt;a name=&quot;content&quot;&gt;Contents&lt;/a&gt; 
- [Exercise 1: KNN classification with numpy and sklearn](#ex1) 
- [Exercise 2: Code acceleration with cython](#ex2) 
- [Exercise 3: Code acceleration with numba](#ex3) 
--- 
</span><span class="s0">#%% 
</span><span class="s2">%</span><span class="s1">load_ext autoreload</span>
<span class="s2">%</span><span class="s1">autoreload </span><span class="s3">2</span>
<span class="s0">#%% md 
</span><span class="s1">## &lt;a name=&quot;ex1&quot;&gt;Exercise 1: K-Nearest Neighbours (K-NN) classification with numpy and scikit-learn&lt;/a&gt; [(&amp;#8593;)](#content) 
</span><span class="s0">#%% md 
</span><span class="s1">This session is a first introduction to classification using the most intuitive non parametric method: the $K$-nearest neighbours. The principle is [the following](https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn%20classification). A set of labelled observations is given as a learning set. A classification taks then consists in assigning a label to any new observation. In particular, the K-NN approach consists in assigning to the observation the most frequent label among its $K$ nearest neighbours taken in the training set. 
</span><span class="s0">#%% md 
</span><span class="s1">### A. Validation on synthetic data 
 
Load the training and test datasets `data/synth_train.txt` and `data/synth_test.txt`. Targets belong to the set $\{1,2\}$ and entries belong to $\mathbb{R}^2$. The file `data/synth_train.txt` contain 100 training data samples, and `data/synth_test.txt` contains 200 test samples, where: 
 
- the 1st column contains the label of the class the sample; 
- columns 2 &amp; 3 contain the coordinates of each sample (in $\mathbb{R}^2$). 
 
Useful commands can be found below. 
</span><span class="s0">#%% md 
</span><span class="s1">```python 
# load the training set 
train = np.loadtxt('data/synth_train.txt')  #...,delimiter=',') if there are ',' as delimiters 
class_train = train[:,0] 
x_train = train[:,1:] 
N_train = train.shape[0] 
``` 
</span><span class="s0">#%% md 
</span><span class="s1">```python 
# load the test set 
test = np.loadtxt('/datasynth_test.txt')  
class_test_1 = test[test[:,0]==1] 
class_test_2 = test[test[:,0]==2] 
x_test = test[:,1:] 
N_test = test.shape[0] 
``` 
</span><span class="s0">#%% md 
</span><span class="s1">1\. Display the training set and distinguish the two classes.  
 
&gt; Hint: useful functions include `matplotlib.pyplot.scatter` or `matplotlib.pyplot.plot`. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">import </span><span class="s1">numpy </span><span class="s4">as </span><span class="s1">np</span>
<span class="s4">import </span><span class="s1">matplotlib</span><span class="s2">.</span><span class="s1">pyplot </span><span class="s4">as </span><span class="s1">plt</span>
<span class="s4">import </span><span class="s1">bottleneck </span><span class="s4">as </span><span class="s1">bn</span>
<span class="s4">import </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">neighbors</span>
<span class="s4">import </span><span class="s1">timeit</span>
<span class="s0">#%% 
# load the training set</span>
<span class="s1">train </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">loadtxt</span><span class="s2">(</span><span class="s5">'data/synth_train.txt'</span><span class="s2">)  </span><span class="s0">#...,delimiter=',') if there are ',' as delimiters</span>
<span class="s1">class_train </span><span class="s2">= </span><span class="s1">train</span><span class="s2">[:,</span><span class="s3">0</span><span class="s2">]</span>
<span class="s1">x_train </span><span class="s2">= </span><span class="s1">train</span><span class="s2">[:,</span><span class="s3">1</span><span class="s2">:]</span>
<span class="s1">N_train </span><span class="s2">= </span><span class="s1">train</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>

<span class="s0"># load the test set</span>
<span class="s1">test </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">loadtxt</span><span class="s2">(</span><span class="s5">'data/synth_test.txt'</span><span class="s2">) </span>
<span class="s1">class_test </span><span class="s2">= </span><span class="s1">test</span><span class="s2">[:,</span><span class="s3">0</span><span class="s2">]</span>
<span class="s0">#class_test_1 = test[test[:,0]==1]</span>
<span class="s0">#class_test_2 = test[test[:,0]==2]</span>
<span class="s1">x_test </span><span class="s2">= </span><span class="s1">test</span><span class="s2">[:,</span><span class="s3">1</span><span class="s2">:]</span>
<span class="s1">N_test </span><span class="s2">= </span><span class="s1">test</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>

<span class="s0">#Plot the training set with different colors for each class</span>
<span class="s1">plt</span><span class="s2">.</span><span class="s1">scatter</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">[:,</span><span class="s3">0</span><span class="s2">],</span><span class="s1">x_train</span><span class="s2">[:,</span><span class="s3">1</span><span class="s2">],</span><span class="s1">c</span><span class="s2">=</span><span class="s1">class_train</span><span class="s2">)</span>
<span class="s1">plt</span><span class="s2">.</span><span class="s1">show</span><span class="s2">()</span>
<span class="s0">#%% md 
</span><span class="s1">Thanks to the option `c=class_train` of `plt.scatter`, we can easily distinguish the two classes. 
</span><span class="s0">#%% md 
</span><span class="s1">2\. Implement the K-nearest neighbours algorithm for classification. 
 
&gt; Hint:  
&gt; - useful functions include `numpy.linalg.norm`, `numpy.argsort`, `numpy.bincount`; 
&gt; - implement the algorithm as a function rather than an object. This will drastically simplify the acceleration step using Cython. 
&gt; - for an optimized partial sorting procedure, you may have a look at the [`bottleneck.argpartition` function](https://bottleneck.readthedocs.io/en/latest/reference.html#bottleneck.argpartition). 
&gt; 1. Compute for each row in `x_test` (if necessary use `np.newaxis`) its distance with respect to `x_train`: 
&gt;  - Use  `numpy.linalg.norm` (in which dimension this distance is computed ? Consider using `axis` argument) 
&gt; 2. Sort the ordered collection of distances (indices from smallest to largest (in ascending order) by the distances): 
&gt;   - Use `np.argsort` (at the end replace this procedure by `bottleneck.argpartition`) 
&gt;   - Once the sorting is done, we take only the indices of `labels` of the `n_neighbours` nearest neighbours of the `class_train` : 
&gt;     - `id = np.argsort(distances)[:n_ neighbours]` and `labels = class_train[id]` 
&gt; 3. The K-nearest can be used for **Regression**, in this case it is necessary to return the mean of the K-labels. For **Classification**,  we return the mode of the K-labels : 
&gt; - Use `np.bincount` for `labels` to affect the variable `class_pred[q]` (for row `q`). This procedure counts the number of occurrences of each value in array. **Mode** is the value that appears. How can we get this value ? 
 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">def </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">,</span><span class="s1">x_test</span><span class="s2">,</span><span class="s1">n_neighbours</span><span class="s2">):</span>
    <span class="s1">N_train </span><span class="s2">= </span><span class="s1">x_train</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">N_test </span><span class="s2">= </span><span class="s1">x_test</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">distances </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s1">N_test</span><span class="s2">,</span><span class="s1">N_train</span><span class="s2">))</span>
    <span class="s4">for </span><span class="s1">i </span><span class="s4">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">N_test</span><span class="s2">):</span>
        <span class="s0">#Compute the distance between each row of x_test and x_train</span>
        <span class="s1">distances</span><span class="s2">[</span><span class="s1">i</span><span class="s2">,:] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">-</span><span class="s1">x_test</span><span class="s2">[</span><span class="s1">i</span><span class="s2">,:],</span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
    <span class="s0">#Sort the distances and keep the indices of the nearest neighbours</span>
    <span class="s1">id </span><span class="s2">= </span><span class="s1">bn</span><span class="s2">.</span><span class="s1">argpartition</span><span class="s2">(</span><span class="s1">distances</span><span class="s2">,</span><span class="s1">n_neighbours</span><span class="s2">-</span><span class="s3">1</span><span class="s2">,</span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)[:,:</span><span class="s1">n_neighbours</span><span class="s2">]</span>
    <span class="s0">#Retrieve the labels of the nearest neighbours</span>
    <span class="s1">labels </span><span class="s2">= </span><span class="s1">class_train</span><span class="s2">[</span><span class="s1">id</span><span class="s2">]</span>
    <span class="s1">class_pred </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">N_test</span><span class="s2">)</span>
    <span class="s4">for </span><span class="s1">i </span><span class="s4">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">N_test</span><span class="s2">):</span>
        <span class="s0">#Give to class_pred the most frequent label among the nearest neighbours</span>
        <span class="s1">class_pred</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">argmax</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">bincount</span><span class="s2">(</span><span class="s1">labels</span><span class="s2">[</span><span class="s1">i</span><span class="s2">,:].</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">int</span><span class="s2">)))</span>
    <span class="s4">return </span><span class="s1">class_pred</span>

<span class="s1">class_pred </span><span class="s2">= </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">,</span><span class="s1">x_test</span><span class="s2">,</span><span class="s3">5</span><span class="s2">)</span>
<span class="s1">plt</span><span class="s2">.</span><span class="s1">scatter</span><span class="s2">(</span><span class="s1">x_test</span><span class="s2">[:,</span><span class="s3">0</span><span class="s2">],</span><span class="s1">x_test</span><span class="s2">[:,</span><span class="s3">1</span><span class="s2">],</span><span class="s1">c</span><span class="s2">=</span><span class="s1">class_pred</span><span class="s2">)</span>
<span class="s1">plt</span><span class="s2">.</span><span class="s1">show</span><span class="s2">()</span>
<span class="s0">#%% md 
</span><span class="s1">3\. Compute the error rate on the training set and the test set for $K \in \{1,2, \dotsc, 20\}$. Display the classification result (see 1.) for the configuration with the lowest error rate. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">for </span><span class="s1">k </span><span class="s4">in </span><span class="s1">range</span><span class="s2">(</span><span class="s3">1</span><span class="s2">,</span><span class="s3">21</span><span class="s2">):</span>
    <span class="s1">class_pred </span><span class="s2">= </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">,</span><span class="s1">x_test</span><span class="s2">,</span><span class="s1">k</span><span class="s2">)</span>
    <span class="s1">error_rate </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">class_pred</span><span class="s2">!=</span><span class="s1">class_test</span><span class="s2">)/</span><span class="s1">N_test</span>
    <span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Error rate for k =&quot;</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s5">&quot;:&quot;</span><span class="s2">, </span><span class="s1">error_rate</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">4\. Comment on your results. Which value of $K$ seems optimal ? 
 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% md 
</span><span class="s1">The value of $K$ that seems optimal here is $K=3$, for which we get the smallest error rate (0.045). 
</span><span class="s0">#%% md 
</span><span class="s1">5\. Compare the results of you implementation with those of [`sklearn.neighbors.KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier). Compare the runtime of these two versions using the [`timeit`](https://docs.python.org/3/library/timeit.html) module (see session 1). 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">for </span><span class="s1">k </span><span class="s4">in </span><span class="s1">range</span><span class="s2">(</span><span class="s3">1</span><span class="s2">,</span><span class="s3">21</span><span class="s2">):</span>
    <span class="s1">knn_classifier </span><span class="s2">= </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">neighbors</span><span class="s2">.</span><span class="s1">KNeighborsClassifier</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)</span>
    <span class="s1">knn_classifier</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">)</span>
    <span class="s1">class_pred </span><span class="s2">= </span><span class="s1">knn_classifier</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">x_test</span><span class="s2">)</span>
    <span class="s1">error_rate </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">class_pred</span><span class="s2">!=</span><span class="s1">class_test</span><span class="s2">)/</span><span class="s1">N_test</span>
    <span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Error rate for k =&quot;</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s5">&quot;:&quot;</span><span class="s2">, </span><span class="s1">error_rate</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">We have exactly the same results when using the algorithm from `scikit-learn`. Now let's compare the running time of both algorithms (for 100 iterations): 
</span><span class="s0">#%% 
</span><span class="s1">my_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">,</span><span class="s1">x_test</span><span class="s2">,</span><span class="s3">5</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">sklearn_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">neighbors</span><span class="s2">.</span><span class="s1">KNeighborsClassifier</span><span class="s2">(</span><span class="s3">5</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">).</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">x_test</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Our KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">my_knn_time</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Scikit-learn KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">sklearn_knn_time</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">Quite surprisingly, our algorithm is faster than the one from `scikit-learn` (around 0.24s vs 69s for 100 iterations). This might be due to the fact that our dataset is small (only 2 features and 300 samples), and that the `scikit-learn` algorithm is more general. 
</span><span class="s0">#%% md 
</span><span class="s1">### B. Application to a real dataset (Breast cancer Wisconsin). 
 
6\. Apply the K-NN classifier to the real dataset `data/wdbc12.data.txt.` Further details about the data are provided in `data/wdbc12.names.txt`. 
 
&gt; Hint: you can use the function [`train_test_split` from `sklearn.model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into a training and a test set. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
# Load the dataset</span>
<span class="s1">data </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">loadtxt</span><span class="s2">(</span><span class="s5">'data/wdbc12.data.txt'</span><span class="s2">,</span><span class="s1">delimiter</span><span class="s2">=</span><span class="s5">','</span><span class="s2">)</span>
<span class="s1">X </span><span class="s2">= </span><span class="s1">data</span><span class="s2">[:,</span><span class="s3">2</span><span class="s2">:]</span>
<span class="s1">y </span><span class="s2">= </span><span class="s1">data</span><span class="s2">[:,</span><span class="s3">1</span><span class="s2">]</span>
<span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">model_selection</span><span class="s2">.</span><span class="s1">train_test_split</span><span class="s2">(</span><span class="s1">X</span><span class="s2">,</span><span class="s1">y</span><span class="s2">,</span><span class="s1">test_size</span><span class="s2">=</span><span class="s3">0.2</span><span class="s2">)</span>

<span class="s0">#Apply the KNN classifier to the dataset with n_neighbours=5</span>
<span class="s1">y_pred1 </span><span class="s2">= </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">,</span><span class="s1">y_train</span><span class="s2">,</span><span class="s1">X_test</span><span class="s2">,</span><span class="s3">5</span><span class="s2">)</span>
<span class="s1">error_rate </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">y_pred1</span><span class="s2">!=</span><span class="s1">y_test</span><span class="s2">)/</span><span class="s1">y_test</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Error rate for our KNN classifier:&quot;</span><span class="s2">, </span><span class="s1">error_rate</span><span class="s2">)</span>

<span class="s1">knn_classifier </span><span class="s2">= </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">neighbors</span><span class="s2">.</span><span class="s1">KNeighborsClassifier</span><span class="s2">(</span><span class="s3">5</span><span class="s2">)</span>
<span class="s1">knn_classifier</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">,</span><span class="s1">y_train</span><span class="s2">)</span>
<span class="s1">y_pred2 </span><span class="s2">= </span><span class="s1">knn_classifier</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">)</span>
<span class="s1">error_rate </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">y_pred2</span><span class="s2">!=</span><span class="s1">y_test</span><span class="s2">)/</span><span class="s1">y_test</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Error rate for sklearn KNN classifier:&quot;</span><span class="s2">, </span><span class="s1">error_rate</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">We have again exaclty the same results for both algorithms. Now let's compare the running time of both algorithms (for 100 iterations): 
</span><span class="s0">#%% 
</span><span class="s1">my_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">,</span><span class="s1">y_train</span><span class="s2">,</span><span class="s1">X_test</span><span class="s2">,</span><span class="s3">5</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">sklearn_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">neighbors</span><span class="s2">.</span><span class="s1">KNeighborsClassifier</span><span class="s2">(</span><span class="s3">5</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">,</span><span class="s1">y_train</span><span class="s2">).</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Our KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">my_knn_time</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Scikit-learn KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">sklearn_knn_time</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">We see that for a larger, more complex dataset, the `scikit-learn` algorithm is faster than ours (around 0.19s vs 1.12s for 100 iterations), which is what we could expect. 
</span><span class="s0">#%% md 
</span><span class="s1">## &lt;a name=&quot;ex2&quot;&gt;Exercise 2: Code acceleration with cython&lt;/a&gt; [(&amp;#8593;)](#content) 
 
Cython allows C code to be easily interfaced with Python. It can be useful to make your code faster for a small coding effort, in particular when using loops. A general approach to optimize your code is outlined in the [Scipy lecture notes, Section 2.4](https://scipy-lectures.org/advanced/optimizing/index.html). Complementary reading about interfacing Python with C can be found in [Section 2.8](https://scipy-lectures.org/advanced/interfacing_with_c/interfacing_with_c.html). 
 
1\. Read carefully the [cython tutorial](http://docs.cython.org/en/latest/src/tutorial/cython_tutorial.html), which describes step by the step how the toy example reported below has been developed. 
</span><span class="s0">#%% md 
</span><span class="s1">**Setup**: Compile the toy example provided in `example_cy/` by running, in the command line (anaconda prompt on windows) 
</span><span class="s0">#%% md 
</span><span class="s1">```bash 
cd example_cy &amp;&amp; python setup.py build_ext --inplace 
``` 
</span><span class="s0">#%% md 
</span><span class="s1">Note that the compilation process has been slightly automatised with the instructions reported in `example_cy/setup.py`. To test the module, run 
</span><span class="s0">#%% 
# !cd example_cy &amp;&amp; python setup.py build_ext --inplace</span>
<span class="s0">#%% 
</span><span class="s4">import </span><span class="s1">example_cy</span><span class="s2">.</span><span class="s1">helloworld </span><span class="s4">as </span><span class="s1">toy</span>

<span class="s1">toy</span><span class="s2">.</span><span class="s1">printhello</span><span class="s2">()</span>
<span class="s0">#%% md 
</span><span class="s1">which should display 
```python 
Hello World 
``` 
</span><span class="s0">#%% md 
</span><span class="s1">&gt; Warning:  
&gt; - do not forget to include an empty `__init__.py` file in the directory where your source code lives (`import` will fail if this is not the case). 
&gt; - in case you have any setup issue, take a look at the `notes.md` file. 
&gt; - if the C code and/or the executable do not seem to be regenerated by the build instructions, delete the C code and the executable first, and re-execute the compilation afterwards. 
&gt; - do not hesitate to restart the Python kernel if necessary when the Cython executable has been re-generated. 
</span><span class="s0">#%% md 
</span><span class="s1">2\. Read the [Numpy/Cython tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial), focussing on the paragraphs **Cython at a glance**, and **Your Cython environment** until **&quot;More generic code&quot;**. An example to compile a `.pyx` file depending on `numpy` is included in `example_np_cy/`. 
</span><span class="s0">#%% md 
</span><span class="s1">&gt; Remarks:  
&gt; - the `annotate=True` flag in the `setup.py` allows an additional `.html` document to be generated (`&lt;your_module_name&gt;.html`), showing, for each line of the Cython code, the associated C instructions generated. Highlighted in yellow are the interactions with Python: the darker a region appears, the less efficient the generated C code is for this section. Work in priority on these!  
&gt; - make sure all the previously generated files are deleted to allow the .html report to be generated; 
&gt; - if you are working on your own machine and don't have a C/C++ compiler installed, read the notes provided in `notes.md`; 
&gt; - use `cdef` for pure C functions (not exported to Python), `cpdef` should be favored for functions containing C instructions and later called from Python. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">import </span><span class="s1">example_np_cy</span><span class="s2">.</span><span class="s1">compute_cy </span><span class="s4">as </span><span class="s1">cc</span>

<span class="s1">result </span><span class="s2">= </span><span class="s1">cc</span><span class="s2">.</span><span class="s1">compute</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">], [</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">]],</span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">'int32'</span><span class="s2">),</span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">], [</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">]],</span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">'int32'</span><span class="s2">),</span><span class="s3">1</span><span class="s2">,</span><span class="s3">2</span><span class="s2">,</span><span class="s3">3</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s1">result</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">3\. Use Cython to implement a faster version of the numpy K-NN classifier implemented in [Exercise 1](#ex1). To do so, apply step-by-step the techniques introduced in the [Numpy/Cython tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial) (*i.e.*, compile and time your code after each step to report the evolution, keeping track of the different versions of the cython function). 
 
&gt; Hint: if you keep numpy arrays, make sure you use memory views (see numpy/cython tutorial) to access the elements within it. Be extremely careful with the type of the input arrays (you may need to recast the format of the input elements before entering the function. The `numpy.asarray` function can prove useful). 
 
&gt; **Detailed guidelines**: a few notes and *caveat* to help you re-writing your code in cython: 
&gt; - try to reduce the number of calls to numpy instructions as much as possible; 
&gt; - **you do not have to optimize everything**. For the KNN function above, most of the time is spent in computing euclidean distances: you can thus focus on optimizing tihs operations by explicitly writing a for loop, which will ensure a minimal interaction with numpy when generating the associated C code at compilation. Calls to other numpy functions can be kept as-is; 
&gt; - if you need to create an array within the cython function, used np.zeros (**do NOT use python lists**), and use a memory view to access its content; 
&gt; - specify the type for all variables and numpy arrays. Pay attention to the type of the input arrays passed to the Cython function; 
&gt; - whenever an array is returned, use memory views and index(es) to efficiently access its content; 
&gt; - some numpy operators (e.g., broadcasting mechanism) do not work with memory views. In this case, you can directly write for loop(s) to encode the operation of interest (the loops will be optimized out at compile time); 
&gt; - only use at the final development stage the following cython optimization (not before, as they can crash the program without any help): 
&gt; 
&gt;```python 
&gt;@cython.boundscheck(False) 
&gt;@cython.wraparound(False) 
&gt;``` 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% md 
</span><span class="s1">Cf. `knn_np_cy/knn_cy.pyx` and `knn_np_cy/setup.py` 
</span><span class="s0">#%% md 
</span><span class="s1">4\. Compare the runtime of the two algorithms (using `timeit.timeit`), and conclude about the interest of using cython in this case. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">import </span><span class="s1">knn_np_cy</span><span class="s2">.</span><span class="s1">knn_cy </span><span class="s4">as </span><span class="s1">knn_cy</span>

<span class="s1">cy_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">knn_cy</span><span class="s2">.</span><span class="s1">KNN_cy</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">),</span><span class="s1">class_train</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">),</span><span class="s1">x_test</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">),</span><span class="s3">5</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">py_knn_time </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">KNN</span><span class="s2">(</span><span class="s1">x_train</span><span class="s2">,</span><span class="s1">class_train</span><span class="s2">,</span><span class="s1">x_test</span><span class="s2">,</span><span class="s3">5</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Python KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">py_knn_time</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Cython KNN algorithm running time:&quot;</span><span class="s2">, </span><span class="s1">cy_knn_time</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">The cython version of our KNN algorithm is approximately 20% faster than the classical implementation (0.48s vs 0.61s) for 100 executions. It is already a significant acceleration, though the time required to write the cython code is not negligible. However, for more important and complex algorithms that require a lot of computations, the improvement will probably be way more interesting. 
</span><span class="s0">#%% md 
</span><span class="s1">## &lt;a name=&quot;ex3&quot;&gt;Exercise 3: Code acceleration with numba&lt;/a&gt; [(&amp;#8593;)](#content) 
 
`numba` is a just-in-time (JIT) compiler which translates Python codes into efficient machine code at runtime. A significant acceleration can be obtained by adding a few simple decorators to a standard Python function, up to a few restrictions detailed [here](http://numba.pydata.org/numba-doc/latest/user/performance-tips.html). 
 
If you have written most of the KNN classifier of exercise 1 with numpy, there is little to no chance that you will get an acceleration with numba (justifying the use of cython in this case). An interesting acceleration factor can however be obtained for the computation of the total variation investigated in session 2. 
</span><span class="s0">#%% md 
</span><span class="s1">1\. Take a look at the [numba 5 min tour](http://numba.pydata.org/numba-doc/latest/user/5minguide.html), and accelerate the total variation code from session 2 with the `@jit` decorator. You may have to rewrite small portions of your code to get the expected acceleration (see [performance tips](http://numba.pydata.org/numba-doc/latest/user/performance-tips.html)). 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
</span><span class="s4">import </span><span class="s1">numba </span><span class="s4">as </span><span class="s1">nb</span>
<span class="s4">import </span><span class="s1">timeit</span>
<span class="s0">#%% 
#Basic function from Lab2</span>
<span class="s4">def </span><span class="s1">tv</span><span class="s2">(</span><span class="s1">X</span><span class="s2">):</span>
    <span class="s4">assert </span><span class="s1">X</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">&lt; </span><span class="s3">3</span><span class="s2">, </span><span class="s5">&quot;Error: the input array has more than 2 dimensions&quot;</span>
    <span class="s1">X_h </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">c_</span><span class="s2">[</span><span class="s1">np</span><span class="s2">.</span><span class="s1">diff</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">), </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">])] </span><span class="s0">#Horizontal differences</span>
    <span class="s1">X_v </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">r_</span><span class="s2">[</span><span class="s1">np</span><span class="s2">.</span><span class="s1">diff</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">), </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s3">1</span><span class="s2">, </span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]))] </span><span class="s0">#Vertical differences</span>
    <span class="s1">tv </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">X_h</span><span class="s2">**</span><span class="s3">2 </span><span class="s2">+ </span><span class="s1">X_v</span><span class="s2">**</span><span class="s3">2</span><span class="s2">))</span>
    <span class="s4">return </span><span class="s1">tv</span>
<span class="s0">#%% 
#Function with the @jit decorator (we replaced the np.diff function with axis argument since it is not supported by numba)</span>
<span class="s2">@</span><span class="s1">nb</span><span class="s2">.</span><span class="s1">jit</span><span class="s2">(</span><span class="s1">nopython</span><span class="s2">=</span><span class="s4">True</span><span class="s2">)</span>
<span class="s4">def </span><span class="s1">tv_jit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">):</span>
    <span class="s4">assert </span><span class="s1">X</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">&lt; </span><span class="s3">3</span><span class="s2">, </span><span class="s5">&quot;Error: the input array has more than 2 dimensions&quot;</span>
    <span class="s1">X_h </span><span class="s2">= </span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">:] - </span><span class="s1">X</span><span class="s2">[:, :-</span><span class="s3">1</span><span class="s2">] </span>
    <span class="s1">X_h </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">hstack</span><span class="s2">((</span><span class="s1">X_h</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">], </span><span class="s3">1</span><span class="s2">)))) </span><span class="s0"># Horizontal differences</span>
    <span class="s1">X_v </span><span class="s2">= </span><span class="s1">X</span><span class="s2">[</span><span class="s3">1</span><span class="s2">:, :] - </span><span class="s1">X</span><span class="s2">[:-</span><span class="s3">1</span><span class="s2">, :]</span>
    <span class="s1">X_v </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">vstack</span><span class="s2">((</span><span class="s1">X_v</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s3">1</span><span class="s2">, </span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]))))  </span><span class="s0"># Vertical differences</span>
    <span class="s1">tv </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">X_h</span><span class="s2">**</span><span class="s3">2 </span><span class="s2">+ </span><span class="s1">X_v</span><span class="s2">**</span><span class="s3">2</span><span class="s2">))</span>
    <span class="s4">return </span><span class="s1">tv</span>
<span class="s0">#%% md 
</span><span class="s1">2\. Compare the runtime of the your numpy implementation and the `numba`-accelerated version (using `timeit.timeit`).  
&gt; **Warning**: first run the numba version once to trigger the compilation, and then time it as usual. This is needed to avoid including the JIT compilation step in the runtime. 
</span><span class="s0">#%% md 
</span><span class="s1">**Answer:** 
</span><span class="s0">#%% 
#Trigger the compilation</span>
<span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s3">1000</span><span class="s2">,</span><span class="s3">1000</span><span class="s2">)</span>
<span class="s1">tv_jit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>
<span class="s0">#%% 
</span><span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s3">1000</span><span class="s2">,</span><span class="s3">1000</span><span class="s2">)</span>
<span class="s1">tv_normal </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">tv</span><span class="s2">(</span><span class="s1">X</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">tv_numba </span><span class="s2">= </span><span class="s1">timeit</span><span class="s2">.</span><span class="s1">timeit</span><span class="s2">(</span><span class="s4">lambda</span><span class="s2">: </span><span class="s1">tv_jit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">),</span><span class="s1">number</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Running time of the function without the @jit decorator:&quot;</span><span class="s2">, </span><span class="s1">tv_normal</span><span class="s2">)</span>
<span class="s1">print</span><span class="s2">(</span><span class="s5">&quot;Running time of the function with the @jit decorator:&quot;</span><span class="s2">, </span><span class="s1">tv_numba</span><span class="s2">)</span>
<span class="s0">#%% md 
</span><span class="s1">We see that the `numba`-accelerated version is approximately 1 second faster than the numpy version, which represent a significant acceleration (30% of the total execution time), for little to no adjustements (apart from a decorator) compared to the cython version.</span></pre>
</body>
</html>